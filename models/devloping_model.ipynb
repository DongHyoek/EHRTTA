{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed9ed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\korea\\miniconda3\\envs\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, LlamaModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c729b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\" \n",
    "\n",
    "hf_config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "# hidden state 뽑기 좋은 AutoModel 사용\n",
    "backbone = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",)\n",
    "\n",
    "        # 1) backbone freeze\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "backbone.eval()\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# padding='max_length', max_length = 4800\n",
    "\n",
    "print(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3b9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DownstreamConfig:\n",
    "    model_id: str = \"meta-llama/Meta-Llama-3.1-8B\"  # base로 쓰는 걸 추천(인스트럭트도 가능)\n",
    "    num_labels: int = 2\n",
    "    task: str = \"classification\"  # \"classification\" or \"regression\"\n",
    "    pooling: str = \"last\"         # \"last\" or \"mean\"\n",
    "    dora_r: int = 8\n",
    "    dora_alpha: int = 16\n",
    "    dora_dropout: float = 0.0\n",
    "    target_modules: tuple = (\"q_proj\", \"v_proj\")  # attention에만\n",
    "    torch_dtype: torch.dtype = torch.bfloat16\n",
    "\n",
    "args = DownstreamConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66383622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 설정\n",
    "peft_args = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    r=args.dora_r,\n",
    "    lora_alpha=args.dora_alpha,\n",
    "    lora_dropout=args.dora_dropout,\n",
    "    target_modules=list(args.target_modules),\n",
    "    bias=\"none\",\n",
    "    use_dora=True,)\n",
    "\n",
    "peft_backbone = get_peft_model(backbone, peft_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf794be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\korea\\miniconda3\\envs\\.venv\\lib\\site-packages\\peft\\mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Users\\korea\\miniconda3\\envs\\.venv\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_backbone = get_peft_model(backbone, peft_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ffd600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.embed_tokens.weight torch.Size([128256, 4096]) False\n",
      "base_model.model.layers.0.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.0.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.0.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.0.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.1.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.1.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.1.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.1.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.2.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.2.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.2.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.2.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.3.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.3.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.3.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.3.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.4.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.4.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.4.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.4.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.5.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.5.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.5.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.5.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.6.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.6.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.6.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.6.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.7.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.7.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.7.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.7.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.8.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.8.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.8.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.8.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.9.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.9.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.9.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.9.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.10.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.10.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.10.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.10.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.11.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.11.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.11.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.11.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.12.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.12.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.12.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.12.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.13.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.13.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.13.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.13.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.14.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.14.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.14.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.14.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.15.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.15.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.15.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.15.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.16.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.16.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.16.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.16.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.17.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.17.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.17.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.17.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.18.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.18.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.18.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.18.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.19.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.19.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.19.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.19.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.20.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.20.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.20.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.20.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.21.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.21.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.21.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.21.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.22.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.22.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.22.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.22.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.23.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.23.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.23.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.23.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.24.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.24.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.24.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.24.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.25.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.25.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.25.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.25.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.26.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.26.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.26.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.26.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.27.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.27.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.27.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.27.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.28.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.28.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.28.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.28.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.29.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.29.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.29.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.29.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.30.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.30.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.30.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.30.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.31.self_attn.q_proj.base_layer.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) True\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_magnitude_vector.default.weight torch.Size([4096]) True\n",
      "base_model.model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.31.self_attn.v_proj.base_layer.weight torch.Size([1024, 4096]) False\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) True\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) True\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_magnitude_vector.default.weight torch.Size([1024]) True\n",
      "base_model.model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096]) False\n",
      "base_model.model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096]) False\n",
      "base_model.model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336]) False\n",
      "base_model.model.layers.31.input_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.layers.31.post_attention_layernorm.weight torch.Size([4096]) False\n",
      "base_model.model.norm.weight torch.Size([4096]) False\n"
     ]
    }
   ],
   "source": [
    "for name, p in peft_backbone.named_parameters():\n",
    "    print(name, p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed15c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "base_model\n",
      "base_model.model\n",
      "base_model.model.embed_tokens\n",
      "base_model.model.layers\n",
      "base_model.model.layers.0\n",
      "base_model.model.layers.0.self_attn\n",
      "base_model.model.layers.0.self_attn.q_proj\n",
      "base_model.model.layers.0.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.0.self_attn.k_proj\n",
      "base_model.model.layers.0.self_attn.v_proj\n",
      "base_model.model.layers.0.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.0.self_attn.o_proj\n",
      "base_model.model.layers.0.mlp\n",
      "base_model.model.layers.0.mlp.gate_proj\n",
      "base_model.model.layers.0.mlp.up_proj\n",
      "base_model.model.layers.0.mlp.down_proj\n",
      "base_model.model.layers.0.mlp.act_fn\n",
      "base_model.model.layers.0.input_layernorm\n",
      "base_model.model.layers.0.post_attention_layernorm\n",
      "base_model.model.layers.1\n",
      "base_model.model.layers.1.self_attn\n",
      "base_model.model.layers.1.self_attn.q_proj\n",
      "base_model.model.layers.1.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.1.self_attn.k_proj\n",
      "base_model.model.layers.1.self_attn.v_proj\n",
      "base_model.model.layers.1.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.1.self_attn.o_proj\n",
      "base_model.model.layers.1.mlp\n",
      "base_model.model.layers.1.mlp.gate_proj\n",
      "base_model.model.layers.1.mlp.up_proj\n",
      "base_model.model.layers.1.mlp.down_proj\n",
      "base_model.model.layers.1.mlp.act_fn\n",
      "base_model.model.layers.1.input_layernorm\n",
      "base_model.model.layers.1.post_attention_layernorm\n",
      "base_model.model.layers.2\n",
      "base_model.model.layers.2.self_attn\n",
      "base_model.model.layers.2.self_attn.q_proj\n",
      "base_model.model.layers.2.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.2.self_attn.k_proj\n",
      "base_model.model.layers.2.self_attn.v_proj\n",
      "base_model.model.layers.2.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.2.self_attn.o_proj\n",
      "base_model.model.layers.2.mlp\n",
      "base_model.model.layers.2.mlp.gate_proj\n",
      "base_model.model.layers.2.mlp.up_proj\n",
      "base_model.model.layers.2.mlp.down_proj\n",
      "base_model.model.layers.2.mlp.act_fn\n",
      "base_model.model.layers.2.input_layernorm\n",
      "base_model.model.layers.2.post_attention_layernorm\n",
      "base_model.model.layers.3\n",
      "base_model.model.layers.3.self_attn\n",
      "base_model.model.layers.3.self_attn.q_proj\n",
      "base_model.model.layers.3.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.3.self_attn.k_proj\n",
      "base_model.model.layers.3.self_attn.v_proj\n",
      "base_model.model.layers.3.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.3.self_attn.o_proj\n",
      "base_model.model.layers.3.mlp\n",
      "base_model.model.layers.3.mlp.gate_proj\n",
      "base_model.model.layers.3.mlp.up_proj\n",
      "base_model.model.layers.3.mlp.down_proj\n",
      "base_model.model.layers.3.mlp.act_fn\n",
      "base_model.model.layers.3.input_layernorm\n",
      "base_model.model.layers.3.post_attention_layernorm\n",
      "base_model.model.layers.4\n",
      "base_model.model.layers.4.self_attn\n",
      "base_model.model.layers.4.self_attn.q_proj\n",
      "base_model.model.layers.4.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.4.self_attn.k_proj\n",
      "base_model.model.layers.4.self_attn.v_proj\n",
      "base_model.model.layers.4.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.4.self_attn.o_proj\n",
      "base_model.model.layers.4.mlp\n",
      "base_model.model.layers.4.mlp.gate_proj\n",
      "base_model.model.layers.4.mlp.up_proj\n",
      "base_model.model.layers.4.mlp.down_proj\n",
      "base_model.model.layers.4.mlp.act_fn\n",
      "base_model.model.layers.4.input_layernorm\n",
      "base_model.model.layers.4.post_attention_layernorm\n",
      "base_model.model.layers.5\n",
      "base_model.model.layers.5.self_attn\n",
      "base_model.model.layers.5.self_attn.q_proj\n",
      "base_model.model.layers.5.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.5.self_attn.k_proj\n",
      "base_model.model.layers.5.self_attn.v_proj\n",
      "base_model.model.layers.5.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.5.self_attn.o_proj\n",
      "base_model.model.layers.5.mlp\n",
      "base_model.model.layers.5.mlp.gate_proj\n",
      "base_model.model.layers.5.mlp.up_proj\n",
      "base_model.model.layers.5.mlp.down_proj\n",
      "base_model.model.layers.5.mlp.act_fn\n",
      "base_model.model.layers.5.input_layernorm\n",
      "base_model.model.layers.5.post_attention_layernorm\n",
      "base_model.model.layers.6\n",
      "base_model.model.layers.6.self_attn\n",
      "base_model.model.layers.6.self_attn.q_proj\n",
      "base_model.model.layers.6.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.6.self_attn.k_proj\n",
      "base_model.model.layers.6.self_attn.v_proj\n",
      "base_model.model.layers.6.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.6.self_attn.o_proj\n",
      "base_model.model.layers.6.mlp\n",
      "base_model.model.layers.6.mlp.gate_proj\n",
      "base_model.model.layers.6.mlp.up_proj\n",
      "base_model.model.layers.6.mlp.down_proj\n",
      "base_model.model.layers.6.mlp.act_fn\n",
      "base_model.model.layers.6.input_layernorm\n",
      "base_model.model.layers.6.post_attention_layernorm\n",
      "base_model.model.layers.7\n",
      "base_model.model.layers.7.self_attn\n",
      "base_model.model.layers.7.self_attn.q_proj\n",
      "base_model.model.layers.7.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.7.self_attn.k_proj\n",
      "base_model.model.layers.7.self_attn.v_proj\n",
      "base_model.model.layers.7.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.7.self_attn.o_proj\n",
      "base_model.model.layers.7.mlp\n",
      "base_model.model.layers.7.mlp.gate_proj\n",
      "base_model.model.layers.7.mlp.up_proj\n",
      "base_model.model.layers.7.mlp.down_proj\n",
      "base_model.model.layers.7.mlp.act_fn\n",
      "base_model.model.layers.7.input_layernorm\n",
      "base_model.model.layers.7.post_attention_layernorm\n",
      "base_model.model.layers.8\n",
      "base_model.model.layers.8.self_attn\n",
      "base_model.model.layers.8.self_attn.q_proj\n",
      "base_model.model.layers.8.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.8.self_attn.k_proj\n",
      "base_model.model.layers.8.self_attn.v_proj\n",
      "base_model.model.layers.8.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.8.self_attn.o_proj\n",
      "base_model.model.layers.8.mlp\n",
      "base_model.model.layers.8.mlp.gate_proj\n",
      "base_model.model.layers.8.mlp.up_proj\n",
      "base_model.model.layers.8.mlp.down_proj\n",
      "base_model.model.layers.8.mlp.act_fn\n",
      "base_model.model.layers.8.input_layernorm\n",
      "base_model.model.layers.8.post_attention_layernorm\n",
      "base_model.model.layers.9\n",
      "base_model.model.layers.9.self_attn\n",
      "base_model.model.layers.9.self_attn.q_proj\n",
      "base_model.model.layers.9.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.9.self_attn.k_proj\n",
      "base_model.model.layers.9.self_attn.v_proj\n",
      "base_model.model.layers.9.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.9.self_attn.o_proj\n",
      "base_model.model.layers.9.mlp\n",
      "base_model.model.layers.9.mlp.gate_proj\n",
      "base_model.model.layers.9.mlp.up_proj\n",
      "base_model.model.layers.9.mlp.down_proj\n",
      "base_model.model.layers.9.mlp.act_fn\n",
      "base_model.model.layers.9.input_layernorm\n",
      "base_model.model.layers.9.post_attention_layernorm\n",
      "base_model.model.layers.10\n",
      "base_model.model.layers.10.self_attn\n",
      "base_model.model.layers.10.self_attn.q_proj\n",
      "base_model.model.layers.10.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.10.self_attn.k_proj\n",
      "base_model.model.layers.10.self_attn.v_proj\n",
      "base_model.model.layers.10.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.10.self_attn.o_proj\n",
      "base_model.model.layers.10.mlp\n",
      "base_model.model.layers.10.mlp.gate_proj\n",
      "base_model.model.layers.10.mlp.up_proj\n",
      "base_model.model.layers.10.mlp.down_proj\n",
      "base_model.model.layers.10.mlp.act_fn\n",
      "base_model.model.layers.10.input_layernorm\n",
      "base_model.model.layers.10.post_attention_layernorm\n",
      "base_model.model.layers.11\n",
      "base_model.model.layers.11.self_attn\n",
      "base_model.model.layers.11.self_attn.q_proj\n",
      "base_model.model.layers.11.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.11.self_attn.k_proj\n",
      "base_model.model.layers.11.self_attn.v_proj\n",
      "base_model.model.layers.11.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.11.self_attn.o_proj\n",
      "base_model.model.layers.11.mlp\n",
      "base_model.model.layers.11.mlp.gate_proj\n",
      "base_model.model.layers.11.mlp.up_proj\n",
      "base_model.model.layers.11.mlp.down_proj\n",
      "base_model.model.layers.11.mlp.act_fn\n",
      "base_model.model.layers.11.input_layernorm\n",
      "base_model.model.layers.11.post_attention_layernorm\n",
      "base_model.model.layers.12\n",
      "base_model.model.layers.12.self_attn\n",
      "base_model.model.layers.12.self_attn.q_proj\n",
      "base_model.model.layers.12.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.12.self_attn.k_proj\n",
      "base_model.model.layers.12.self_attn.v_proj\n",
      "base_model.model.layers.12.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.12.self_attn.o_proj\n",
      "base_model.model.layers.12.mlp\n",
      "base_model.model.layers.12.mlp.gate_proj\n",
      "base_model.model.layers.12.mlp.up_proj\n",
      "base_model.model.layers.12.mlp.down_proj\n",
      "base_model.model.layers.12.mlp.act_fn\n",
      "base_model.model.layers.12.input_layernorm\n",
      "base_model.model.layers.12.post_attention_layernorm\n",
      "base_model.model.layers.13\n",
      "base_model.model.layers.13.self_attn\n",
      "base_model.model.layers.13.self_attn.q_proj\n",
      "base_model.model.layers.13.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.13.self_attn.k_proj\n",
      "base_model.model.layers.13.self_attn.v_proj\n",
      "base_model.model.layers.13.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.13.self_attn.o_proj\n",
      "base_model.model.layers.13.mlp\n",
      "base_model.model.layers.13.mlp.gate_proj\n",
      "base_model.model.layers.13.mlp.up_proj\n",
      "base_model.model.layers.13.mlp.down_proj\n",
      "base_model.model.layers.13.mlp.act_fn\n",
      "base_model.model.layers.13.input_layernorm\n",
      "base_model.model.layers.13.post_attention_layernorm\n",
      "base_model.model.layers.14\n",
      "base_model.model.layers.14.self_attn\n",
      "base_model.model.layers.14.self_attn.q_proj\n",
      "base_model.model.layers.14.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.14.self_attn.k_proj\n",
      "base_model.model.layers.14.self_attn.v_proj\n",
      "base_model.model.layers.14.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.14.self_attn.o_proj\n",
      "base_model.model.layers.14.mlp\n",
      "base_model.model.layers.14.mlp.gate_proj\n",
      "base_model.model.layers.14.mlp.up_proj\n",
      "base_model.model.layers.14.mlp.down_proj\n",
      "base_model.model.layers.14.mlp.act_fn\n",
      "base_model.model.layers.14.input_layernorm\n",
      "base_model.model.layers.14.post_attention_layernorm\n",
      "base_model.model.layers.15\n",
      "base_model.model.layers.15.self_attn\n",
      "base_model.model.layers.15.self_attn.q_proj\n",
      "base_model.model.layers.15.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.15.self_attn.k_proj\n",
      "base_model.model.layers.15.self_attn.v_proj\n",
      "base_model.model.layers.15.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.15.self_attn.o_proj\n",
      "base_model.model.layers.15.mlp\n",
      "base_model.model.layers.15.mlp.gate_proj\n",
      "base_model.model.layers.15.mlp.up_proj\n",
      "base_model.model.layers.15.mlp.down_proj\n",
      "base_model.model.layers.15.mlp.act_fn\n",
      "base_model.model.layers.15.input_layernorm\n",
      "base_model.model.layers.15.post_attention_layernorm\n",
      "base_model.model.layers.16\n",
      "base_model.model.layers.16.self_attn\n",
      "base_model.model.layers.16.self_attn.q_proj\n",
      "base_model.model.layers.16.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.16.self_attn.k_proj\n",
      "base_model.model.layers.16.self_attn.v_proj\n",
      "base_model.model.layers.16.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.16.self_attn.o_proj\n",
      "base_model.model.layers.16.mlp\n",
      "base_model.model.layers.16.mlp.gate_proj\n",
      "base_model.model.layers.16.mlp.up_proj\n",
      "base_model.model.layers.16.mlp.down_proj\n",
      "base_model.model.layers.16.mlp.act_fn\n",
      "base_model.model.layers.16.input_layernorm\n",
      "base_model.model.layers.16.post_attention_layernorm\n",
      "base_model.model.layers.17\n",
      "base_model.model.layers.17.self_attn\n",
      "base_model.model.layers.17.self_attn.q_proj\n",
      "base_model.model.layers.17.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.17.self_attn.k_proj\n",
      "base_model.model.layers.17.self_attn.v_proj\n",
      "base_model.model.layers.17.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.17.self_attn.o_proj\n",
      "base_model.model.layers.17.mlp\n",
      "base_model.model.layers.17.mlp.gate_proj\n",
      "base_model.model.layers.17.mlp.up_proj\n",
      "base_model.model.layers.17.mlp.down_proj\n",
      "base_model.model.layers.17.mlp.act_fn\n",
      "base_model.model.layers.17.input_layernorm\n",
      "base_model.model.layers.17.post_attention_layernorm\n",
      "base_model.model.layers.18\n",
      "base_model.model.layers.18.self_attn\n",
      "base_model.model.layers.18.self_attn.q_proj\n",
      "base_model.model.layers.18.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.18.self_attn.k_proj\n",
      "base_model.model.layers.18.self_attn.v_proj\n",
      "base_model.model.layers.18.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.18.self_attn.o_proj\n",
      "base_model.model.layers.18.mlp\n",
      "base_model.model.layers.18.mlp.gate_proj\n",
      "base_model.model.layers.18.mlp.up_proj\n",
      "base_model.model.layers.18.mlp.down_proj\n",
      "base_model.model.layers.18.mlp.act_fn\n",
      "base_model.model.layers.18.input_layernorm\n",
      "base_model.model.layers.18.post_attention_layernorm\n",
      "base_model.model.layers.19\n",
      "base_model.model.layers.19.self_attn\n",
      "base_model.model.layers.19.self_attn.q_proj\n",
      "base_model.model.layers.19.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.19.self_attn.k_proj\n",
      "base_model.model.layers.19.self_attn.v_proj\n",
      "base_model.model.layers.19.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.19.self_attn.o_proj\n",
      "base_model.model.layers.19.mlp\n",
      "base_model.model.layers.19.mlp.gate_proj\n",
      "base_model.model.layers.19.mlp.up_proj\n",
      "base_model.model.layers.19.mlp.down_proj\n",
      "base_model.model.layers.19.mlp.act_fn\n",
      "base_model.model.layers.19.input_layernorm\n",
      "base_model.model.layers.19.post_attention_layernorm\n",
      "base_model.model.layers.20\n",
      "base_model.model.layers.20.self_attn\n",
      "base_model.model.layers.20.self_attn.q_proj\n",
      "base_model.model.layers.20.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.20.self_attn.k_proj\n",
      "base_model.model.layers.20.self_attn.v_proj\n",
      "base_model.model.layers.20.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.20.self_attn.o_proj\n",
      "base_model.model.layers.20.mlp\n",
      "base_model.model.layers.20.mlp.gate_proj\n",
      "base_model.model.layers.20.mlp.up_proj\n",
      "base_model.model.layers.20.mlp.down_proj\n",
      "base_model.model.layers.20.mlp.act_fn\n",
      "base_model.model.layers.20.input_layernorm\n",
      "base_model.model.layers.20.post_attention_layernorm\n",
      "base_model.model.layers.21\n",
      "base_model.model.layers.21.self_attn\n",
      "base_model.model.layers.21.self_attn.q_proj\n",
      "base_model.model.layers.21.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.21.self_attn.k_proj\n",
      "base_model.model.layers.21.self_attn.v_proj\n",
      "base_model.model.layers.21.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.21.self_attn.o_proj\n",
      "base_model.model.layers.21.mlp\n",
      "base_model.model.layers.21.mlp.gate_proj\n",
      "base_model.model.layers.21.mlp.up_proj\n",
      "base_model.model.layers.21.mlp.down_proj\n",
      "base_model.model.layers.21.mlp.act_fn\n",
      "base_model.model.layers.21.input_layernorm\n",
      "base_model.model.layers.21.post_attention_layernorm\n",
      "base_model.model.layers.22\n",
      "base_model.model.layers.22.self_attn\n",
      "base_model.model.layers.22.self_attn.q_proj\n",
      "base_model.model.layers.22.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.22.self_attn.k_proj\n",
      "base_model.model.layers.22.self_attn.v_proj\n",
      "base_model.model.layers.22.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.22.self_attn.o_proj\n",
      "base_model.model.layers.22.mlp\n",
      "base_model.model.layers.22.mlp.gate_proj\n",
      "base_model.model.layers.22.mlp.up_proj\n",
      "base_model.model.layers.22.mlp.down_proj\n",
      "base_model.model.layers.22.mlp.act_fn\n",
      "base_model.model.layers.22.input_layernorm\n",
      "base_model.model.layers.22.post_attention_layernorm\n",
      "base_model.model.layers.23\n",
      "base_model.model.layers.23.self_attn\n",
      "base_model.model.layers.23.self_attn.q_proj\n",
      "base_model.model.layers.23.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.23.self_attn.k_proj\n",
      "base_model.model.layers.23.self_attn.v_proj\n",
      "base_model.model.layers.23.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.23.self_attn.o_proj\n",
      "base_model.model.layers.23.mlp\n",
      "base_model.model.layers.23.mlp.gate_proj\n",
      "base_model.model.layers.23.mlp.up_proj\n",
      "base_model.model.layers.23.mlp.down_proj\n",
      "base_model.model.layers.23.mlp.act_fn\n",
      "base_model.model.layers.23.input_layernorm\n",
      "base_model.model.layers.23.post_attention_layernorm\n",
      "base_model.model.layers.24\n",
      "base_model.model.layers.24.self_attn\n",
      "base_model.model.layers.24.self_attn.q_proj\n",
      "base_model.model.layers.24.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.24.self_attn.k_proj\n",
      "base_model.model.layers.24.self_attn.v_proj\n",
      "base_model.model.layers.24.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.24.self_attn.o_proj\n",
      "base_model.model.layers.24.mlp\n",
      "base_model.model.layers.24.mlp.gate_proj\n",
      "base_model.model.layers.24.mlp.up_proj\n",
      "base_model.model.layers.24.mlp.down_proj\n",
      "base_model.model.layers.24.mlp.act_fn\n",
      "base_model.model.layers.24.input_layernorm\n",
      "base_model.model.layers.24.post_attention_layernorm\n",
      "base_model.model.layers.25\n",
      "base_model.model.layers.25.self_attn\n",
      "base_model.model.layers.25.self_attn.q_proj\n",
      "base_model.model.layers.25.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.25.self_attn.k_proj\n",
      "base_model.model.layers.25.self_attn.v_proj\n",
      "base_model.model.layers.25.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.25.self_attn.o_proj\n",
      "base_model.model.layers.25.mlp\n",
      "base_model.model.layers.25.mlp.gate_proj\n",
      "base_model.model.layers.25.mlp.up_proj\n",
      "base_model.model.layers.25.mlp.down_proj\n",
      "base_model.model.layers.25.mlp.act_fn\n",
      "base_model.model.layers.25.input_layernorm\n",
      "base_model.model.layers.25.post_attention_layernorm\n",
      "base_model.model.layers.26\n",
      "base_model.model.layers.26.self_attn\n",
      "base_model.model.layers.26.self_attn.q_proj\n",
      "base_model.model.layers.26.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.26.self_attn.k_proj\n",
      "base_model.model.layers.26.self_attn.v_proj\n",
      "base_model.model.layers.26.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.26.self_attn.o_proj\n",
      "base_model.model.layers.26.mlp\n",
      "base_model.model.layers.26.mlp.gate_proj\n",
      "base_model.model.layers.26.mlp.up_proj\n",
      "base_model.model.layers.26.mlp.down_proj\n",
      "base_model.model.layers.26.mlp.act_fn\n",
      "base_model.model.layers.26.input_layernorm\n",
      "base_model.model.layers.26.post_attention_layernorm\n",
      "base_model.model.layers.27\n",
      "base_model.model.layers.27.self_attn\n",
      "base_model.model.layers.27.self_attn.q_proj\n",
      "base_model.model.layers.27.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.27.self_attn.k_proj\n",
      "base_model.model.layers.27.self_attn.v_proj\n",
      "base_model.model.layers.27.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.27.self_attn.o_proj\n",
      "base_model.model.layers.27.mlp\n",
      "base_model.model.layers.27.mlp.gate_proj\n",
      "base_model.model.layers.27.mlp.up_proj\n",
      "base_model.model.layers.27.mlp.down_proj\n",
      "base_model.model.layers.27.mlp.act_fn\n",
      "base_model.model.layers.27.input_layernorm\n",
      "base_model.model.layers.27.post_attention_layernorm\n",
      "base_model.model.layers.28\n",
      "base_model.model.layers.28.self_attn\n",
      "base_model.model.layers.28.self_attn.q_proj\n",
      "base_model.model.layers.28.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.28.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.28.self_attn.k_proj\n",
      "base_model.model.layers.28.self_attn.v_proj\n",
      "base_model.model.layers.28.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.28.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.28.self_attn.o_proj\n",
      "base_model.model.layers.28.mlp\n",
      "base_model.model.layers.28.mlp.gate_proj\n",
      "base_model.model.layers.28.mlp.up_proj\n",
      "base_model.model.layers.28.mlp.down_proj\n",
      "base_model.model.layers.28.mlp.act_fn\n",
      "base_model.model.layers.28.input_layernorm\n",
      "base_model.model.layers.28.post_attention_layernorm\n",
      "base_model.model.layers.29\n",
      "base_model.model.layers.29.self_attn\n",
      "base_model.model.layers.29.self_attn.q_proj\n",
      "base_model.model.layers.29.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.29.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.29.self_attn.k_proj\n",
      "base_model.model.layers.29.self_attn.v_proj\n",
      "base_model.model.layers.29.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.29.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.29.self_attn.o_proj\n",
      "base_model.model.layers.29.mlp\n",
      "base_model.model.layers.29.mlp.gate_proj\n",
      "base_model.model.layers.29.mlp.up_proj\n",
      "base_model.model.layers.29.mlp.down_proj\n",
      "base_model.model.layers.29.mlp.act_fn\n",
      "base_model.model.layers.29.input_layernorm\n",
      "base_model.model.layers.29.post_attention_layernorm\n",
      "base_model.model.layers.30\n",
      "base_model.model.layers.30.self_attn\n",
      "base_model.model.layers.30.self_attn.q_proj\n",
      "base_model.model.layers.30.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.30.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.30.self_attn.k_proj\n",
      "base_model.model.layers.30.self_attn.v_proj\n",
      "base_model.model.layers.30.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.30.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.30.self_attn.o_proj\n",
      "base_model.model.layers.30.mlp\n",
      "base_model.model.layers.30.mlp.gate_proj\n",
      "base_model.model.layers.30.mlp.up_proj\n",
      "base_model.model.layers.30.mlp.down_proj\n",
      "base_model.model.layers.30.mlp.act_fn\n",
      "base_model.model.layers.30.input_layernorm\n",
      "base_model.model.layers.30.post_attention_layernorm\n",
      "base_model.model.layers.31\n",
      "base_model.model.layers.31.self_attn\n",
      "base_model.model.layers.31.self_attn.q_proj\n",
      "base_model.model.layers.31.self_attn.q_proj.base_layer\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_dropout\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_A\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_B\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.layers.31.self_attn.q_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.31.self_attn.k_proj\n",
      "base_model.model.layers.31.self_attn.v_proj\n",
      "base_model.model.layers.31.self_attn.v_proj.base_layer\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_dropout\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_A\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_B\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.layers.31.self_attn.v_proj.lora_magnitude_vector.default\n",
      "base_model.model.layers.31.self_attn.o_proj\n",
      "base_model.model.layers.31.mlp\n",
      "base_model.model.layers.31.mlp.gate_proj\n",
      "base_model.model.layers.31.mlp.up_proj\n",
      "base_model.model.layers.31.mlp.down_proj\n",
      "base_model.model.layers.31.mlp.act_fn\n",
      "base_model.model.layers.31.input_layernorm\n",
      "base_model.model.layers.31.post_attention_layernorm\n",
      "base_model.model.norm\n",
      "base_model.model.rotary_emb\n"
     ]
    }
   ],
   "source": [
    "for name, p in peft_backbone.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31e48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    DoRA/LoRA의 lora_A 출력 (low-rank vector)에 적용할 변환.\n",
    "    예: L2 normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: (B, L, r)\n",
    "        norm = z.norm(p=2, dim=-1, keepdim=True).clamp_min(self.eps)\n",
    "        return z / norm\n",
    "\n",
    "\n",
    "class AWithTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    기존 lora_A(Linear)를 감싸서 A(x) -> transform(A(x)) 반환\n",
    "    \"\"\"\n",
    "    def __init__(self, a_linear: nn.Module, transform: nn.Module):\n",
    "        super().__init__()\n",
    "        self.a_linear = a_linear\n",
    "        self.transform = transform\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.a_linear(x)\n",
    "        z = self.transform(z)\n",
    "        return z\n",
    "\n",
    "def wrap_all_lora_A_modules_inplace(peft_model: nn.Module, transform: nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    PEFT가 삽입한 LoRA/DoRA 레이어들 중 lora_A 모듈을 찾아 wrapper로 교체.\n",
    "    (PEFT 내부 구현이 버전별로 조금 달라서 '탐색 기반'으로 최대한 견고하게 작성)\n",
    "\n",
    "    Returns:\n",
    "        교체한 lora_A 개수\n",
    "    \"\"\"\n",
    "    replaced = 0\n",
    "\n",
    "    for module in peft_model.modules():\n",
    "        # PEFT LoRA layer들은 보통 lora_A, lora_B 같은 attribute를 가짐(버전/타겟 레이어 타입별로 약간 다름)\n",
    "        if hasattr(module, \"lora_A\"):\n",
    "            lora_A = getattr(module, \"lora_A\")\n",
    "\n",
    "            # lora_A가 adapter_name -> nn.Module 형태로 담긴 dict/ModuleDict인 경우가 흔함\n",
    "            if isinstance(lora_A, (nn.ModuleDict, dict)):\n",
    "                for adapter_name, a_mod in list(lora_A.items()):\n",
    "                    # 이미 wrapper면 skip\n",
    "                    if isinstance(a_mod, AWithTransform):\n",
    "                        continue\n",
    "                    lora_A[adapter_name] = AWithTransform(a_mod, transform)\n",
    "                    replaced += 1\n",
    "\n",
    "            # 일부 구현에서는 단일 모듈일 수도 있으니 방어적으로 처리\n",
    "            elif isinstance(lora_A, nn.Module) and not isinstance(lora_A, AWithTransform):\n",
    "                setattr(module, \"lora_A\", AWithTransform(lora_A, transform))\n",
    "                replaced += 1\n",
    "\n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a6f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaModel(\n",
      "      (embed_tokens): Embedding(128256, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x LlamaDecoderLayer(\n",
      "          (self_attn): LlamaAttention(\n",
      "            (q_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): AWithTransform(\n",
      "                  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  (transform): FeatureTransform()\n",
      "                )\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict(\n",
      "                (default): lora.dora.DoraLinearLayer()\n",
      "              )\n",
      "            )\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): AWithTransform(\n",
      "                  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  (transform): FeatureTransform()\n",
      "                )\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict(\n",
      "                (default): lora.dora.DoraLinearLayer()\n",
      "              )\n",
      "            )\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "-------\n",
      "LoraModel(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): lora.Linear(\n",
      "            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): AWithTransform(\n",
      "                (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                (transform): FeatureTransform()\n",
      "              )\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "            (lora_magnitude_vector): ModuleDict(\n",
      "              (default): lora.dora.DoraLinearLayer()\n",
      "            )\n",
      "          )\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): lora.Linear(\n",
      "            (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): AWithTransform(\n",
      "                (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                (transform): FeatureTransform()\n",
      "              )\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "            (lora_magnitude_vector): ModuleDict(\n",
      "              (default): lora.dora.DoraLinearLayer()\n",
      "            )\n",
      "          )\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): lora.Linear(\n",
      "          (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): AWithTransform(\n",
      "              (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "              (transform): FeatureTransform()\n",
      "            )\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "          (lora_magnitude_vector): ModuleDict(\n",
      "            (default): lora.dora.DoraLinearLayer()\n",
      "          )\n",
      "        )\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): lora.Linear(\n",
      "          (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): AWithTransform(\n",
      "              (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "              (transform): FeatureTransform()\n",
      "            )\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "          (lora_magnitude_vector): ModuleDict(\n",
      "            (default): lora.dora.DoraLinearLayer()\n",
      "          )\n",
      "        )\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "-------\n",
      "Embedding(128256, 4096)\n",
      "-------\n",
      "ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Identity()\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): AWithTransform(\n",
      "            (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "            (transform): FeatureTransform()\n",
      "          )\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict(\n",
      "          (default): lora.dora.DoraLinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Identity()\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): AWithTransform(\n",
      "            (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "            (transform): FeatureTransform()\n",
      "          )\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict(\n",
      "          (default): lora.dora.DoraLinearLayer()\n",
      "        )\n",
      "      )\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      ")\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "FeatureTransform()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): lora.Linear(\n",
      "      (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (lora_dropout): ModuleDict(\n",
      "        (default): Identity()\n",
      "      )\n",
      "      (lora_A): ModuleDict(\n",
      "        (default): AWithTransform(\n",
      "          (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (transform): FeatureTransform()\n",
      "        )\n",
      "      )\n",
      "      (lora_B): ModuleDict(\n",
      "        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "      )\n",
      "      (lora_embedding_A): ParameterDict()\n",
      "      (lora_embedding_B): ParameterDict()\n",
      "      (lora_magnitude_vector): ModuleDict(\n",
      "        (default): lora.dora.DoraLinearLayer()\n",
      "      )\n",
      "    )\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "-------\n",
      "LlamaAttention(\n",
      "  (q_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): lora.Linear(\n",
      "    (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Identity()\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): AWithTransform(\n",
      "        (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        (transform): FeatureTransform()\n",
      "      )\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "    (lora_magnitude_vector): ModuleDict(\n",
      "      (default): lora.dora.DoraLinearLayer()\n",
      "    )\n",
      "  )\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=4096, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "lora.Linear(\n",
      "  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (lora_dropout): ModuleDict(\n",
      "    (default): Identity()\n",
      "  )\n",
      "  (lora_A): ModuleDict(\n",
      "    (default): AWithTransform(\n",
      "      (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "      (transform): FeatureTransform()\n",
      "    )\n",
      "  )\n",
      "  (lora_B): ModuleDict(\n",
      "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "  )\n",
      "  (lora_embedding_A): ParameterDict()\n",
      "  (lora_embedding_B): ParameterDict()\n",
      "  (lora_magnitude_vector): ModuleDict(\n",
      "    (default): lora.dora.DoraLinearLayer()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=1024, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Identity()\n",
      ")\n",
      "-------\n",
      "Identity()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): AWithTransform(\n",
      "    (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "    (transform): FeatureTransform()\n",
      "  )\n",
      ")\n",
      "-------\n",
      "AWithTransform(\n",
      "  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "  (transform): FeatureTransform()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=8, bias=False)\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      ")\n",
      "-------\n",
      "Linear(in_features=8, out_features=1024, bias=False)\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ParameterDict()\n",
      "-------\n",
      "ModuleDict(\n",
      "  (default): lora.dora.DoraLinearLayer()\n",
      ")\n",
      "-------\n",
      "lora.dora.DoraLinearLayer()\n",
      "-------\n",
      "Linear(in_features=4096, out_features=4096, bias=False)\n",
      "-------\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=4096, out_features=14336, bias=False)\n",
      "-------\n",
      "Linear(in_features=14336, out_features=4096, bias=False)\n",
      "-------\n",
      "SiLU()\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "-------\n",
      "LlamaRotaryEmbedding()\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for module in peft_backbone.modules():\n",
    "    print(module)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraAAdaIN(nn.Module):\n",
    "    \"\"\"\n",
    "    lora_A 출력 z를 AdaIN처럼 변환:\n",
    "      z_hat = (z - mu_c) / sigma_c * sigma_t + mu_t\n",
    "    - mu_c, sigma_c: 현재 배치(또는 sample)에서 계산\n",
    "    - mu_t, sigma_t: Dataset A에서 수집한 target stats\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        a_linear: nn.Module,\n",
    "        target_payload: Dict[str, Any],\n",
    "        key: str,\n",
    "        style_mode: Literal[\"aggregate\", \"distribution\"] = \"aggregate\",\n",
    "        selection: Literal[\"mean_of_dist\", \"random\", \"cycle\"] = \"mean_of_dist\",\n",
    "        seed: int = 0,\n",
    "        eps: float = 1e-6,\n",
    "        instance_wise: bool = True,  # AdaIN 느낌(샘플별)로 할지, 배치 전체로 할지\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.a_linear = a_linear\n",
    "        self.key = key\n",
    "        self.style_mode = style_mode\n",
    "        self.selection = selection\n",
    "        self.eps = eps\n",
    "        self.instance_wise = instance_wise\n",
    "\n",
    "        self._rng = random.Random(seed)\n",
    "        self._cycle_idx = 0\n",
    "\n",
    "        # payload 구조:\n",
    "        # target_payload = {\"mode\":..., \"r\":..., \"data\":...}\n",
    "        self._data = target_payload[\"data\"]\n",
    "        self._payload_mode = target_payload[\"mode\"]\n",
    "\n",
    "        if self.style_mode != self._payload_mode:\n",
    "            raise ValueError(\n",
    "                f\"style_mode({self.style_mode})와 payload mode({self._payload_mode})가 다릅니다.\"\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_target_stats(self, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.style_mode == \"aggregate\":\n",
    "            entry: LayerStatsAggregate = self._data[self.key]\n",
    "            mu_t = entry.mean.to(device)\n",
    "            var_t = entry.var.to(device)\n",
    "            return mu_t, var_t\n",
    "\n",
    "        # distribution\n",
    "        entry: LayerStatsDistribution = self._data[self.key]\n",
    "        means, vars_ = entry.means, entry.vars\n",
    "\n",
    "        if len(means) == 0:\n",
    "            raise RuntimeError(f\"[{self.key}] distribution stats가 비어 있습니다.\")\n",
    "\n",
    "        if self.selection == \"mean_of_dist\":\n",
    "            mu_t = torch.stack([m.to(device) for m in means], dim=0).mean(dim=0)\n",
    "            var_t = torch.stack([v.to(device) for v in vars_], dim=0).mean(dim=0)\n",
    "            return mu_t, var_t\n",
    "\n",
    "        if self.selection == \"random\":\n",
    "            i = self._rng.randrange(len(means))\n",
    "            return means[i].to(device), vars_[i].to(device)\n",
    "\n",
    "        # cycle\n",
    "        i = self._cycle_idx % len(means)\n",
    "        self._cycle_idx += 1\n",
    "        return means[i].to(device), vars_[i].to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.a_linear(x)  # (B,L,r) 또는 (B,r)\n",
    "        if z.dim() == 2:\n",
    "            z = z.unsqueeze(1)  # (B,1,r)\n",
    "\n",
    "        device = z.device\n",
    "        mu_t, var_t = self._get_target_stats(device)\n",
    "        sigma_t = torch.sqrt(var_t + self.eps)  # (r,)\n",
    "\n",
    "        # content stats (현재 입력에서)\n",
    "        if self.instance_wise:\n",
    "            # 샘플별로 L축 평균/분산: (B,1,r)\n",
    "            mu_c = z.mean(dim=1, keepdim=True)\n",
    "            var_c = z.var(dim=1, keepdim=True, unbiased=False).clamp_min(1e-12)\n",
    "        else:\n",
    "            # 배치 전체(B*L) 기준 채널별 stats: (1,1,r)\n",
    "            z2 = z.reshape(-1, z.size(-1))\n",
    "            mu_c = z2.mean(dim=0).view(1, 1, -1)\n",
    "            var_c = z2.var(dim=0, unbiased=False).view(1, 1, -1).clamp_min(1e-12)\n",
    "\n",
    "        sigma_c = torch.sqrt(var_c + self.eps)\n",
    "\n",
    "        # broadcast: (r,) -> (1,1,r)\n",
    "        mu_t = mu_t.view(1, 1, -1)\n",
    "        sigma_t = sigma_t.view(1, 1, -1)\n",
    "\n",
    "        z_hat = (z - mu_c) / sigma_c * sigma_t + mu_t\n",
    "\n",
    "        # 원래가 (B,r)였으면 다시 squeeze\n",
    "        if z_hat.size(1) == 1 and x.dim() == 2:\n",
    "            z_hat = z_hat.squeeze(1)\n",
    "        return z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c5fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Wrapped lora_A modules: 0\n",
      "PeftModelForFeatureExtraction(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaModel(\n",
      "      (embed_tokens): Embedding(128256, 4096)\n",
      "      (layers): ModuleList(\n",
      "        (0-31): 32 x LlamaDecoderLayer(\n",
      "          (self_attn): LlamaAttention(\n",
      "            (q_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): AWithTransform(\n",
      "                  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  (transform): FeatureTransform()\n",
      "                )\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict(\n",
      "                (default): lora.dora.DoraLinearLayer()\n",
      "              )\n",
      "            )\n",
      "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): lora.Linear(\n",
      "              (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): AWithTransform(\n",
      "                  (a_linear): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                  (transform): FeatureTransform()\n",
      "                )\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict(\n",
      "                (default): lora.dora.DoraLinearLayer()\n",
      "              )\n",
      "            )\n",
      "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "          (mlp): LlamaMLP(\n",
      "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3) A 출력(low-rank vector) 변형 삽입\n",
    "n = wrap_all_lora_A_modules_inplace(peft_backbone, FeatureTransform())\n",
    "print(f\"[INFO] Wrapped lora_A modules: {n}\")\n",
    "print(peft_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7473d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in peft_backbone.modules():\n",
    "    print(module)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
